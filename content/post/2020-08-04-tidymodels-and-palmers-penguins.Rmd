---
title: "Tidymodels & palmer-penguins"
author: "SNTag"
date: '2020-08-04T00:00:00Z'
output:
  html_document:
    number_sections: false
    toc: false
    toc_depth: 3
    fig_caption: true
    fig_width: 7
    fig_height: 4
---

Oddly enough, I've been hearing a lot about tidymodels this week, from reddit to youtube. Plus, the 
tidytuesday dataset this week looks fun.I figure I'll spend a tidytuesday trying to figure out this 
new and in-development machine-learning package.

For the newcomer, tidymodels is the planned replacement for the caret package. Complicated machine
learning (ml) models will continue using caret for a while, because tidymodels is very new to the
field. I'm taking the word of others more experienced in the matter, but tidymodels does have the
same abilities as caret. This does not speak to tidymodels potential, for it has successfully tied a
large number of ml models under one roof. It's use of tidy implementation means it could also make
ml an easier field to break into for the non-computer scientist like me (I'm the classic biologist
turned programmer).

This is my exploration into the models, some of my notes, and my implementation of tidymodels. I've
referred to a number of sites to get a broad idea. I've put those references at the very
bottom. This will be useful to those new to tidymodels or machine learning.

# the tidymodel packages

`tidymodels` isn't just one package. It's a family of packages, each serving a distinct
purpose. I focus in this post on getting familiar with the essentials; rsample, parsnip, and yardstick.

- rsample : Processes data to prepare it for models.

- recipes : used to create 'recipes' for models. Can be used to design preprocessing matrixs, and
  useful for reproducible research.

- parsnip : For running the models. Attempts to provide a singular interface to multiple
  packages. If you haven't realized, the name is also a pun on the original 'caret' package!

- tune : For fine tuning the models. Can be helpful in getting a model the way you need it. not
  attempted here.

- yardstick : helps in quantifiying models.

I began my machine learning journey with caret, which is wonderfully well-built. Any advanced user
could easily develop sophisticated scripts. Tidymodels has an advantage over caret that I appreciate
as someone still learning ml; it is refreshingly easy to change models, metrics, or other details
during experimentation.

# palmer penguins

When I came across tidymodels, this week's tidytuesday dataset were the palmer penguins. It is
proposed as an alternative to the classic iris dataset.

Below, I show a simple ml model to classify penguins based on body features. I hope it is of help to someone.

## data exploration

```{R packages}
pacman::p_load(magrittr,
               tidyverse,
               ggplot2,
               tidymodels
               )

```

I'm grabbing the data directly from github, but you can also get it from the CRAN [Palmer
Penguins](https://github.com/allisonhorst/palmerpenguins) package.

There are two types of files. The smaller and larger penguins dataset. I'll make use of
the smaller penguins to minimize the variables while playing with tidymodels.

```{R prepping_data}
penguins     <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')

penguins %>% dim
summary(penguins)

penguins %>%
    skimr::skim()

```

I need species as a factor. All rows with nas will be removed to enable downstream analysis.

```{R cleaning_data}
penguins$species <- penguins$species %>%
    as.factor()
penguins <- penguins[-(penguins$bill_length_mm %>% is.na() %>% which()),]

```

Some rapid fire exploration. The graphs below suggest that each of the metrics can help identify a
specific penguin. All four metrics measuring the body have a multi-modal distribution. It's obvious
that in every case, a peak is representative of one species. For example, `bill_depth_mm` can
identify Gentoo, `bill_length_mm` for Adelie, while Chinstrap has a strong overlap with either
species in all metrics. I imagine that this overlap will not interfere with the identification.

```{R met_vs_spc, fig.align = "center", fig.cap = "metrics vs species"}
penguins.pivot <- penguins %>%
    pivot_longer(cols = bill_length_mm:body_mass_g,
                 names_to = "metrics",
                 values_to = "values")

penguins.pivot %>%
    ggplot(aes(values, fill = species)) +
    geom_histogram(bins = 20) +
    facet_wrap(~ metrics, scales = "free_x")

```

I don't make use of the pivot functions often enough. They can be incredibly handy.

<!-- ```{R} -->
<!-- penguins.species <- penguins$species %>% unique() -->

<!-- cors.list <- list() -->

<!-- for (i in 1:length(penguins.species)) { -->
<!--     cors.list[[i]] <- penguins %>% -->
<!--         filter(species == penguins.species[i]) %>% -->
<!--         subset(select = c(3:6)) %>% -->
<!--         cor(method = c("spearman")) %>% -->
<!--         corrplot::corrplot() -->
<!-- } -->

<!-- pdf("plots.pdf") -->
<!-- for (i in 1:3) { -->
<!--     print(cors.list[[i]]) -->
<!-- } -->
<!-- dev.off() -->

<!-- ``` -->

## machine learning

Following chunk makes use of rsample to split the dataset into train/test. As this is a small
dataset, lets split 65/35. The vfold_cv is necessary downstream when tuning the model.

```{R data_splitting}
penguins.split <- penguins %>%
    rsample::initial_split(prop = 0.65, strata = species)

penguins.train <- penguins.split %>%
    rsample::training()

penguins.test <- penguins.split %>%
    rsample::testing()

penguins.cv <- penguins.train %>%
    rsample::vfold_cv(v = 10)

```


## building a model

In one of my first real introductions to tidymodel (which was also with the palmer penguins data
set), the author made a large effort to use logistic regression. That is the wrong way to go,
because a logistic is expecting two factors for the response. Linear regression is open to a greater
range of response variables. Multi-class regression is done here with the engine 'rangr' which is
improved over 'randomForest'.

The 'r set_engine' is not a necessity here. Depending on the parsnip chosen, it automatically
sets to rangr. I imagine that for the complicated models, it will be necessary to specify the engine.

The package tune brings a handy 'fit_resample' function. As the name suggests, it adds to the 'fit'
function by adding resampling capabilities. Specifically, it measures metrics over multiple
resamples (folds) for tuning the model.

```{R model_making}
model.fit.prep <- parsnip::rand_forest(mode = "classification") %>%
    parsnip::set_engine("ranger") %>%
    tune::fit_resamples(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,
                        resamples = penguins.cv)

model.fit.prep

```

Now that we have a basic model, lets build a training example.

```{R}
model.fit <- parsnip::rand_forest(mode = "classification") %>%
    parsnip::set_engine("ranger") %>%
    fit(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,
                        data = penguins.train)

model.fit

```

## evaluating classification model

Here, yardstick comes into play.

```{R metrics_study}
penguins.fit.pred <- model.fit %>%
    predict(penguins.test) %>%
    bind_cols(penguins.test)

```

Looking at the metrics, the model is acceptable. Other metrics could be observed
through 'metric_set' which I will not elaborate here. I'm satisfied by looking at
kappa and accuracy.

```{R metrics_study_2}
penguins.fit.pred %>%
    metrics(species, estimate = .pred_class)

```

Region-Over-Curves (ROC) are a popular measure of success. The smaller the ROC, the greater the
ability to identify species. ROC curves of this dataset suggests a very high ability to identity
species with few type 1 or 2 errors. Interestingly, I thought that chinstrap would be difficult to
identify as it always overlapped with one of the other species. This seems to have been a benefit in
identification.

```{R curves_study, fig.align = "center", fig.cap = "ROC"}
penguins.met <- model.fit.peng %>%
    predict(penguins.test, type = "prob") %>%
    bind_cols(penguins.test) %>%
    yardstick::gain_curve(species,
                          .pred_Adelie,
                          .pred_Chinstrap,
                          .pred_Gentoo)

penguins.met %>%
    autoplot() +
    ggpubr::theme_pubclean() +
    theme(strip.background = element_blank())

```

# Summary

As mentioned in the beginning, it is easy to modify the ml approach on the fly. I have enjoyed
working with tidy models, and plan to continue with it for a while. The only regret I have about the
tidymodels is a near lack of documentation. The website is definitely rich in articles, but I am
having a tough time finding the minutia about function details. That is to be expected for the time
being while the package develops further. For those who are not happy with tidymodels, there is the
alternative ml wrapper 'mlr3'. It is as capable as the tidymodels and does not follow the tidy
approach, which some may find attractive. Plus, I've noticed after writing this article that mlr3
has a bigger following than I thought. I plan to eventually post about my experiences with mlr3 vs
tidymodels.

# References

https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/
http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#tune-the-parameters
https://semba-blog.netlify.app/05/09/2020/a-unified-machine-learning-in-r-with-tidymodels/
https://www.youtube.com/watch?v=ImpXawPNCfM
